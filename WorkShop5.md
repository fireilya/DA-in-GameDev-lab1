# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил(а):
- Перминов Илья Андреевич
- РИ220913
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

Ход работы:

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

Так как коэффиент корреляции в машинном обучении это взаимосвязь переменной-предиктора и целевой переменной (Target Variable), то в ходе анализа кода выяснилось. что текущую взаимосвязь между Agent и Target выражает переменная "distanceToTarget", а константным барьером этой взаимосвязи, определяющим дальнейший ход обучения, является коэффициент 1.42

![Alt text](https://sun9-27.userapi.com/impg/k7_JzqS9TBCQFcqndfZEvy-oLHrMIeIG3fNWaw/UTXR90l0X7c.jpg?size=921x160&quality=96&sign=745e26e795ef2bb711badb66c2fc38d9&type=album "Коэффицент корреляции")

В данном случае влияние коэффициента корреляции на обучение следующее: Чем он меньше, тем точнее обучится модель, так как в данном случае он является строгой верхней границей, определяющей вознаграждение. Также стоит учитывать, что уменьшение данного параметра может сильно увеличить время обучения модели


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

визуализация динамики процесса обучения в зависимости от изменяемой величины будет произведена посредством TensorBoard. Для каждой попытки обучения будет предпринято 100 тысяч шагов

1) play_against_latest_model_ratio - вероятность того, что действия агента будут противоположны предыдущей модели. Низкие значения означают, что действия агента будут одинаковы, что затрудняет обучение, в то время как высокие значения могут привести к дистабилизации среды обучения, но позволяют выработать более узкие рамки вариативности поведения, поэтому в этом параметре необходимо искать золотую середину. Принимает значения от 0.0 до 1.0

Значение 0:

![Alt text](https://sun9-67.userapi.com/impg/xk4ibuCD9OxkMWVXRP_nl627up3MbbFTc92Wcg/_5ReqaM3sRI.jpg?size=913x821&quality=96&sign=7fbc289ae070ff0b75983c5027dcaca9&type=album "Environment, play_against_latest_model_ratio = 0.0")

![Alt text](https://sun9-67.userapi.com/impg/1OgyFzp-PV-WZMdu4OPq-1f_qR8ThGlJAqj1wg/MvizRLe4R48.jpg?size=1208x728&quality=96&sign=d2c7cc67a354de0adbc191a35360e307&type=album "Policy, play_against_latest_model_ratio = 0.0")

---
Значение 1:

![Alt text](https://sun9-25.userapi.com/impg/B9isB_KmZq1AITBw0-jbb3s-ktFPHS7N4ekJEw/a6z1m-f2kck.jpg?size=816x808&quality=96&sign=69009e1b172b656c15c03bdd05e1fcf4&type=album "Environment, play_against_latest_model_ratio = 1.0")

![Alt text](https://sun3-20.userapi.com/impg/o1RixjZ78xcbgIgjBdFqx65IOA4yLGeT8eItig/xwjH3WJdlYA.jpg?size=1210x725&quality=96&sign=50089cb51ddfe857242937989cdf1cc9&type=album "Policy, play_against_latest_model_ratio = 1.0")

Как мы можем заметить, при значении 1 график назначения награды ведёт себя крайне не стабильно и имеет скачущую природу, так как агент постоянно пытается менять ход своих действий. Напротив, график награды при нулевом значении крайне пологий, так как агент не предпринимает кардинальных попыток изменить своё поведение

2) time_horizon - количество шагов, которое должен сделать агент, прежде чем он получит награду и будет добавлен в буфер опыта. Если эпизоды обучения слишком велики, то это число уменьшают, но если число будет слишком маленьким, то мы не сможем охватить всё многообразие последовательных шагов агента, что делает обучение не точным или вовсе невозможным, если достижение цели требует большое количество действий от агента.

Значение 32:

![Alt text](https://sun9-33.userapi.com/impg/AmXYR4OSsClLcprB_uewJnirPuPYdsqZkO4bkw/XZfiqy0n7nA.jpg?size=835x802&quality=96&sign=89a2c7a84ceea973797bcfea2ca203aa&type=album "Environment, time_horizon = 32")

![Alt text](https://sun3-21.userapi.com/impg/uku__DgpRTMKnKvbgLGI-SDtiULMZhnlT1PfCw/KjYvm1FtcyQ.jpg?size=1199x723&quality=96&sign=49b6f9c22028fa824e46f9c58c3f4efd&type=album "Policy, time_horizon = 32")

---

Значение 2048:

![Alt text](https://sun9-17.userapi.com/impg/mI5XZipTgLlwp3RejEY295ozoB4d-5nEBZuB5A/x6if7xXrXWk.jpg?size=850x809&quality=96&sign=6850d30b9a8869c94a2629e4b08901a1&type=album "Environment, time_horizon = 2048")

![Alt text](https://sun9-65.userapi.com/impg/kQ4q9EcvV5TV_VfTDMk0EijxZLV83k6Ivgbhxw/ZoSuw_DVek8.jpg?size=1187x731&quality=96&sign=30d333b961cd7adcafbcd55e0674344e&type=album "Policy, time_horizon = 2048")

На графиках видно, что при низком значении time_horizon награждение агента происходит достаточно стабильно, так как для текущей задачи ему не нужно сделать многого (Только дойти до шахты и вернуться обратно), чего нельзя сказать про выскокие значения. При высоких значениях time_horizon награждение агенту выдаётся скачуще, так как агент успевает наделать много лишних действий, что приводит к ухудшению его результата. Это также подтверждает график ошибок, в начале обучения количество ошибок зашкаливает, что придаёт графику более крутую тенденцию на спад

3) extrinsic -> strength - коэффициент кратности награды. Чем он больше, тем более высокая награда выдаётся агенту

Значение 0.25:

![Alt text](https://sun9-67.userapi.com/impg/Cwcpsvk2mRQMF98jp3GJAxjE1gdxuj5CqQeJzw/OZutkyA-a6Y.jpg?size=840x805&quality=96&sign=1e8facbf9f883d9e0a15f0d4ca89a1e1&type=album "Environment, extrinsic -> strength = 0.25")

![Alt text](https://sun9-52.userapi.com/impg/0yEddNJCVFohgQYX0gwNOVA94ORADEqQSyJPqA/9x49yj2zpKc.jpg?size=1211x719&quality=96&sign=9af908ab3b62cf1d4c8de538fd4e6574&type=album "Policy, extrinsic -> strength = 0.25")

---

Значение 10.0:

![Alt text](https://sun9-78.userapi.com/impg/Oaa8Ds1mHFC_VU_rCpjwbsxuvnTiBU8bI0OjQA/_zUNwounJDo.jpg?size=840x791&quality=96&sign=e645c5be802ab4831d0802fd3dd3693a&type=album "Environment, extrinsic -> strength = 10.0")

![Alt text](https://sun9-45.userapi.com/impg/5QICux2yoNfs4BMC4KVb9QdjbpefLdsARUNieg/d4_-suDiLtI.jpg?size=1199x725&quality=96&sign=039e46a39f3ee433b5cf9d9f29965783&type=album "Policy, extrinsic -> strength = 10.0")

Как мы можем видеть, при внутреннем сигнале награды = 1,
внешняя награда строго равна коэффициенту кратности,
но это не значит, что коэффициент кратности можно выставлять любой.
На графиках видно, что график неудачных попыток при высоком значении коэффициента награды
приобретает скачущую природу, хоть и сохраняет тенденцию на снижение с увеличением количества шагов обучения. Это наглядно демонстрирует, что слишком выскоие значение внешней награды приводят к дистабилизации процесса обучения и необходимо подстраивать это значение в зависимости от внутреннего сигнала награждения. 

4) extrinsic -> gamma - коэффициент скидки будущих награждений.
Этот коэффициент определяет, насколько агенту будут важны будущие награды.
Чем этот коэффициент больше, тем большую направленность будут иметь действия агента на получение награды в будущем, а не в настоящем, и наоборот, при маленьких значениях агент будет делать всё возможное, чтобы получить награду здесь и сейчас. Принимает значения находящиеся в интервале (0; 1)

Значение 0.01:

![Alt text](https://sun9-58.userapi.com/impg/ULfv8UXh4YkMLG5t85si6ou8bFwd5j90Livq0g/HrS54sJ9Isk.jpg?size=825x806&quality=96&sign=80a705907c472a53c0079a6c87d304eb&type=album "Environment, extrinsic -> gamma = 0.01")

![Alt text](https://sun9-6.userapi.com/impg/7bHpMRaju6KSWYlw3MwIEw6gIcU5ILjLt1ff1w/6KyAcueR6DQ.jpg?size=1190x717&quality=96&sign=07e9fa29ded877800c9680b2cd4ac63c&type=album "Policy, extrinsic -> gamma = 0.01")

---

Значение 0.99:

![Alt text](https://sun9-25.userapi.com/impg/pW8Db3LeMyL2UQXX-QQ3WDAhen8K3PCY4eQPhg/EDdDyBbq9LE.jpg?size=820x805&quality=96&sign=21d9f4cf1815b8a6109295197454b89e&type=album "Environment, extrinsic -> gamma = 0.99")

![Alt text](https://sun9-68.userapi.com/impg/PwxYlU8LhMN_VUv22gH1c5qZyo95JcgrnenCzw/PA3Oj2pY5s4.jpg?size=1221x722&quality=96&sign=448e9a90e8f0557f4375eb75071d4bc5&type=album "Policy, extrinsic -> gamma = 0.99")

При высоких значениях данного параметра мы можем видеть хоть слегка и запоздавшее (с 20000 шага), но стабильное значение награды равное 1, что демонстрирует работу агента на будущее. Напротив, маленькое значение вызвало нарастающую тенденцию награды фактически с самого начала обучения, так как агент действовал здесь и сейчас.

5) hyperparameters -> epsilon - определяет максимально возможную степень расхождения между прошлой и настоящей политикой агента. Чем меньше значение, тем медленнее будет обучение, вследствие эффекта хождения по кругу, но зато оно будет стабильным. Наоборот, более выскоие значения предполагают более агрессивное и нестабильное обучение

Значение 0.01:

![Alt text](https://sun9-30.userapi.com/impg/b3HpzDZTQ7y19gFc113S4EB2VPPqk2erwZr3Vw/__QDa1CprC8.jpg?size=813x795&quality=96&sign=0b3189743059a890f729b092642fdda9&type=album "Environment, hyperparameters -> epsilon = 0.01")

![Alt text](https://sun9-23.userapi.com/impg/G8C1uXlWKLJb9zGufc0_d72POx3rg4sHPpAnyQ/CxJlpAiDK0o.jpg?size=1197x718&quality=96&sign=06761600a68c1b242a682f7489c170d8&type=album "Policy, hyperparameters -> epsilon = 0.01")

---

Значение 2.0:

![Alt text](https://sun9-29.userapi.com/impg/5KbhVHsG-INxHipZuxvt2vJsNEPN2KBO8xcKNg/kMYNnVn7CMM.jpg?size=856x766&quality=96&sign=74bcc4d6ccf3ae7b787d50aba01b155f&type=album "Environment, hyperparameters -> epsilon = 2.0")

![Alt text](https://sun9-38.userapi.com/impg/rM4QfNT_2JeqllYHgkKOTQWuP3RrPuOzCuaF3w/hHW9QRw9_bk.jpg?size=1187x725&quality=96&sign=23583b88925f135212a6a8251455a512&type=album "Policy, hyperparameters -> epsilon = 2.0")

Как мы можем видеть, низкое значение hyperparameters -> epsilon демонстрирует достаточно стабильный процесс обучения, о чём нам говорят графики награды и ошибок. Высокие же значения вызвали сильную дестабилизацию, из-за чего уменьшилась награда и увеличилось количество ошибок, более того, графики начали слегка скакать. Вызвано это тем, что во втором случае агент постоянно меняет свои действия практически без каких-либо ограничений, что не позволяет ему выработать узкую и стабильную политику поведения, поэтому в значении hyperparameters -> epsilon необходимо искать золотую середину и подбирать его взависимости от условий обучения.

## Задание 3
### Приведите примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Агент, который следует за определённым объектом может быть полезен в разработке всех жанров игр в поджанре погоня (Примеры: Outlast, NeedForSpeed Rivals)

Агент, который следует по определённому маршруту от точки А к точке Б может быть полезен для разработки интерактивных NPC, которые не будут проходить сквозь игрока, если он встанет на их пути, а искать способ обойти его. Также такой агент был бы полезен для создания противников в гонках со строго заданным маршрутом

ML-Agent лучше всего использовать для игр с динамически изменяющимся миром, где от NPC требуется постоянная адаптация к действиям игрока и окружающему миру, либо же, если конечный алгоритм программной реализации будет слишком сложным, громоздским и трудно-читаемым. Напротив, если задача очень простая, а игровой мир является статичным, то в использовании ML-Agent нет смысла и он только нагрузит игру


## Выводы

В ходе лабораторной работы было проведено ознакомелние с программным средством для создания системы машинного обучения ML-Agent и он был интегрирован в Unity. Также была построена визуализация на основе логов в TebsorBoard

Цели лабораторной работы были достигнуты.
## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
